{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c35cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm, trange\n",
    "from ipynb.fs.full.Bert import BertModel, LayerNorm, FuseEmbeddings\n",
    "from ipynb.fs.full.PreTraining import t2n, metric_report\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4745064a",
   "metadata": {},
   "source": [
    "## Fine Tuning\n",
    "The fine tuning step is the last step in G-BERT.  This code is primarily taken straight from the G-Bert Github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f759f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MappingHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MappingHead, self).__init__()\n",
    "        self.dense = nn.Sequential(nn.Linear(300, 300), nn.ReLU())\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.dense(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2b12c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuning(nn.Module):\n",
    "    def __init__(self, data, useGraph):\n",
    "        super(FineTuning, self).__init__()\n",
    "        self.bert = BertModel(len(data[\"vocab\"]), 300, 0.4, useGraph, data[\"all_conditions\"], data[\"all_drugs\"])\n",
    "        self.dense = nn.ModuleList([MappingHead(), MappingHead()])\n",
    "        self.cls = nn.Sequential(nn.Linear(900, 600), nn.ReLU(), nn.Linear(600, len(data[\"multi_visit_drugs\"])))\n",
    "\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def init_bert_weights(self, module):\n",
    "        '''\n",
    "        Taken from https://github.com/huggingface/transformers/blob/78b7debf56efb907c6af767882162050d4fbb294/src/transformers/modeling_utils.py#L1596\n",
    "        '''\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        elif isinstance(module, LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "            \n",
    "    def forward(self, input_ids, rx_labels):\n",
    "        token_types_ids = torch.cat([torch.zeros((1, input_ids.size(1))), torch.ones(\n",
    "            (1, input_ids.size(1)))], dim=0).long().to(input_ids.device)\n",
    "        token_types_ids = token_types_ids.repeat(\n",
    "            1 if input_ids.size(0)//2 == 0 else input_ids.size(0)//2, 1)\n",
    "        # bert_pool: (2*adm, H)\n",
    "        _, bert_pool = self.bert(input_ids, token_types_ids)\n",
    "        loss = 0\n",
    "        bert_pool = bert_pool.view(2, -1, bert_pool.size(1))  # (2, adm, H)\n",
    "        dx_bert_pool = self.dense[0](bert_pool[0])  # (adm, H)\n",
    "        rx_bert_pool = self.dense[1](bert_pool[1])  # (adm, H)\n",
    "\n",
    "        # mean and concat for rx prediction task\n",
    "        rx_logits = []\n",
    "        for i in range(rx_labels.size(0)):\n",
    "            # mean\n",
    "            dx_mean = torch.mean(dx_bert_pool[0:i+1, :], dim=0, keepdim=True)\n",
    "            rx_mean = torch.mean(rx_bert_pool[0:i+1, :], dim=0, keepdim=True)\n",
    "\n",
    "            # concat\n",
    "            concat = torch.cat(\n",
    "                [dx_mean, rx_mean, dx_bert_pool[i+1, :].unsqueeze(dim=0)], dim=-1)\n",
    "            rx_logits.append(self.cls(concat))\n",
    "\n",
    "        rx_logits = torch.cat(rx_logits, dim=0)\n",
    "    \n",
    "        loss = F.binary_cross_entropy_with_logits(rx_logits, rx_labels)\n",
    "        return loss, rx_logits\n",
    "    \n",
    "    def from_pretrained(data, useGraph, outputFileName):\n",
    "        # Instantiate model.\n",
    "        model = FineTuning(data, useGraph)\n",
    "        \n",
    "        weights_path = os.path.join(\"\", outputFileName)\n",
    "        state_dict = torch.load(weights_path)\n",
    "\n",
    "        old_keys = []\n",
    "        new_keys = []\n",
    "        for key in state_dict.keys():\n",
    "            new_key = None\n",
    "            if 'gamma' in key:\n",
    "                new_key = key.replace('gamma', 'weight')\n",
    "            if 'beta' in key:\n",
    "                new_key = key.replace('beta', 'bias')\n",
    "            if new_key:\n",
    "                old_keys.append(key)\n",
    "                new_keys.append(new_key)\n",
    "        for old_key, new_key in zip(old_keys, new_keys):\n",
    "            state_dict[new_key] = state_dict.pop(old_key)\n",
    "\n",
    "        missing_keys = []\n",
    "        unexpected_keys = []\n",
    "        error_msgs = []\n",
    "        # copy state_dict so _load_from_state_dict can modify it\n",
    "        metadata = getattr(state_dict, '_metadata', None)\n",
    "        state_dict = state_dict.copy()\n",
    "        if metadata is not None:\n",
    "            state_dict._metadata = metadata\n",
    "\n",
    "        def load(module, prefix=''):\n",
    "            local_metadata = {} if metadata is None else metadata.get(\n",
    "                prefix[:-1], {})\n",
    "            module._load_from_state_dict(\n",
    "                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n",
    "            for name, child in module._modules.items():\n",
    "                if child is not None:\n",
    "                    load(child, prefix + name + '.')\n",
    "\n",
    "        load(model, prefix='' if hasattr(model, 'bert') else 'bert.')\n",
    "        \n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6599ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tuning(data, train_dataloader, eval_dataloader, test_dataloader, outputFileName, useGraph):\n",
    "    print(\"***** Running Fine Tuning *****\")\n",
    "    model = FineTuning.from_pretrained(data, useGraph, outputFileName)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available()\n",
    "                              and not args.no_cuda else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    model_to_save = model.module if hasattr(\n",
    "        model, 'module') else model  # Only save the model it-self\n",
    "    dx_output_model_file = os.path.join(\n",
    "        '', outputFileName)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "    dx_acc_best, rx_acc_best = 0, 0\n",
    "    acc_name = 'prauc'\n",
    "    \n",
    "    global_step = 0\n",
    "\n",
    "    dx_acc_best, rx_acc_best = 0, 0\n",
    "    acc_name = 'prauc'\n",
    "    dx_history = {'prauc': []}\n",
    "    rx_history = {'prauc': []}\n",
    "\n",
    "    for _ in range(5):\n",
    "        print(\"***** Running training *****\")\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, dx_labels, rx_labels = batch\n",
    "            input_ids, dx_labels, rx_labels = input_ids.squeeze(dim=0), dx_labels.squeeze(dim=0), rx_labels.squeeze(dim=0)\n",
    "            loss, rx_logits = model(input_ids, rx_labels)\n",
    "            loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += 1\n",
    "            nb_tr_steps += 1\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        global_step += 1\n",
    "        print('train/loss:', tr_loss / nb_tr_steps, \" epoch: \", global_step)\n",
    "        \n",
    "        print(\"***** Running eval *****\")\n",
    "        model.eval()\n",
    "        dx_y_preds = []\n",
    "        dx_y_trues = []\n",
    "        rx_y_preds = []\n",
    "        rx_y_trues = []\n",
    "        for eval_input in eval_dataloader:\n",
    "            eval_input = tuple(t.to(device) for t in eval_input)\n",
    "            input_ids, dx_labels, rx_labels = eval_input\n",
    "            input_ids, dx_labels, rx_labels = input_ids.squeeze(), dx_labels.squeeze(), rx_labels.squeeze(dim=0)\n",
    "            with torch.no_grad():\n",
    "                loss, rx_logits = model(input_ids, rx_labels)\n",
    "                rx_y_preds.append(t2n(torch.sigmoid(rx_logits)))\n",
    "                rx_y_trues.append(t2n(rx_labels))\n",
    "\n",
    "        rx_acc_container = metric_report(np.concatenate(rx_y_preds, axis=0), np.concatenate(rx_y_trues, axis=0))\n",
    "        for k, v in rx_acc_container.items():\n",
    "            print(\"eval/\", k, \": \", v, \" epoch: \", global_step)\n",
    "\n",
    "        if rx_acc_container[acc_name] > rx_acc_best:\n",
    "            rx_acc_best = rx_acc_container[acc_name]\n",
    "            # save model\n",
    "            torch.save(model_to_save.state_dict(), dx_output_model_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
