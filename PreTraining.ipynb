{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7feacb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ipynb.fs.full.Bert import BertModel, LayerNorm\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import roc_auc_score, f1_score, average_precision_score\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5295b34c",
   "metadata": {},
   "source": [
    "## Pre-Training\n",
    "The pre-training is a modified version of the usual pretraining tasks. Instead of the masked language model task and next sentence prediction task, G-BERT uses a self-prediction task and a dual-prediction task. This code is primarily taken straight from the G-Bert Github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b18dac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t2n(x):\n",
    "    return x.detach().cpu().numpy()\n",
    "\n",
    "def multi_label_metric(y_gt, y_pred, y_prob):\n",
    "\n",
    "    def jaccard(y_gt, y_pred):\n",
    "        score = []\n",
    "        for b in range(y_gt.shape[0]):\n",
    "            target = np.where(y_gt[b] == 1)[0]\n",
    "            out_list = np.where(y_pred[b] == 1)[0]\n",
    "            inter = set(out_list) & set(target)\n",
    "            union = set(out_list) | set(target)\n",
    "            jaccard_score = 0 if union == 0 else len(inter) / len(union)\n",
    "            score.append(jaccard_score)\n",
    "        return np.mean(score)\n",
    "\n",
    "    def average_prc(y_gt, y_pred):\n",
    "        score = []\n",
    "        for b in range(y_gt.shape[0]):\n",
    "            target = np.where(y_gt[b] == 1)[0]\n",
    "            out_list = np.where(y_pred[b] == 1)[0]\n",
    "            inter = set(out_list) & set(target)\n",
    "            prc_score = 0 if len(out_list) == 0 else len(inter) / len(out_list)\n",
    "            score.append(prc_score)\n",
    "        return score\n",
    "\n",
    "    def average_recall(y_gt, y_pred):\n",
    "        score = []\n",
    "        for b in range(y_gt.shape[0]):\n",
    "            target = np.where(y_gt[b] == 1)[0]\n",
    "            out_list = np.where(y_pred[b] == 1)[0]\n",
    "            inter = set(out_list) & set(target)\n",
    "            recall_score = 0 if len(target) == 0 else len(inter) / len(target)\n",
    "            score.append(recall_score)\n",
    "        return score\n",
    "\n",
    "    def average_f1(average_prc, average_recall):\n",
    "        score = []\n",
    "        for idx in range(len(average_prc)):\n",
    "            if average_prc[idx] + average_recall[idx] == 0:\n",
    "                score.append(0)\n",
    "            else:\n",
    "                score.append(\n",
    "                    2*average_prc[idx]*average_recall[idx] / (average_prc[idx] + average_recall[idx]))\n",
    "        return score\n",
    "\n",
    "    def f1(y_gt, y_pred):\n",
    "        all_micro = []\n",
    "        for b in range(y_gt.shape[0]):\n",
    "            all_micro.append(f1_score(y_gt[b], y_pred[b], average='macro'))\n",
    "        return np.mean(all_micro)\n",
    "\n",
    "    def roc_auc(y_gt, y_prob):\n",
    "        all_micro = []\n",
    "        for b in range(len(y_gt)):\n",
    "            all_micro.append(roc_auc_score(\n",
    "                y_gt[b], y_prob[b], average='macro'))\n",
    "        return np.mean(all_micro)\n",
    "\n",
    "    def precision_auc(y_gt, y_prob):\n",
    "        all_micro = []\n",
    "        for b in range(len(y_gt)):\n",
    "            all_micro.append(average_precision_score(\n",
    "                y_gt[b], y_prob[b], average='macro'))\n",
    "        return np.mean(all_micro)\n",
    "\n",
    "    def precision_at_k(y_gt, y_prob, k=3):\n",
    "        precision = 0\n",
    "        sort_index = np.argsort(y_prob, axis=-1)[:, ::-1][:, :k]\n",
    "        for i in range(len(y_gt)):\n",
    "            TP = 0\n",
    "            for j in range(len(sort_index[i])):\n",
    "                if y_gt[i, sort_index[i, j]] == 1:\n",
    "                    TP += 1\n",
    "            precision += TP / len(sort_index[i])\n",
    "        return precision / len(y_gt)\n",
    "\n",
    "    auc = roc_auc(y_gt, y_prob)\n",
    "    p_1 = precision_at_k(y_gt, y_prob, k=1)\n",
    "    p_3 = precision_at_k(y_gt, y_prob, k=3)\n",
    "    p_5 = precision_at_k(y_gt, y_prob, k=5)\n",
    "    f1 = f1(y_gt, y_pred)\n",
    "    prauc = precision_auc(y_gt, y_prob)\n",
    "    ja = jaccard(y_gt, y_pred)\n",
    "    avg_prc = average_prc(y_gt, y_pred)\n",
    "    avg_recall = average_recall(y_gt, y_pred)\n",
    "    avg_f1 = average_f1(avg_prc, avg_recall)\n",
    "\n",
    "    return ja, prauc, np.mean(avg_prc), np.mean(avg_recall), np.mean(avg_f1)\n",
    "\n",
    "\n",
    "def metric_report(y_pred, y_true, therhold=0.5):\n",
    "    y_prob = y_pred.copy()\n",
    "    y_pred[y_pred > therhold] = 1\n",
    "    y_pred[y_pred <= therhold] = 0\n",
    "\n",
    "    acc_container = {}\n",
    "    ja, prauc, avg_p, avg_r, avg_f1 = multi_label_metric(\n",
    "        y_true, y_pred, y_prob)\n",
    "    acc_container['jaccard'] = ja\n",
    "    acc_container['f1'] = avg_f1\n",
    "    acc_container['prauc'] = prauc\n",
    "\n",
    "    return acc_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0704857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Pretrain(nn.Module):\n",
    "    def __init__(self, data, hidden_size, dropout_prob, useGraph):\n",
    "        super(BERT_Pretrain, self).__init__()\n",
    "        self.all_conditions_size = len(data[\"all_conditions\"])\n",
    "        self.all_drugs_size = len(data[\"all_drugs\"])\n",
    "\n",
    "        self.bert = BertModel(len(data[\"vocab\"]), hidden_size, dropout_prob, useGraph, data[\"all_conditions\"], data[\"all_drugs\"])\n",
    "        \n",
    "        self.cls_conditions_1 = nn.Sequential(nn.Linear(hidden_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, self.all_conditions_size))\n",
    "        \n",
    "        self.cls_drugs_1 = nn.Sequential(nn.Linear(hidden_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, self.all_conditions_size))\n",
    "        \n",
    "        self.cls_conditions_2 = nn.Sequential(nn.Linear(hidden_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, self.all_drugs_size))\n",
    "        \n",
    "        self.cls_drugs_2 = nn.Sequential(nn.Linear(hidden_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, self.all_drugs_size))\n",
    "        \n",
    "        self.apply(self.init_bert_weights)\n",
    "        \n",
    "    def init_bert_weights(self, module):\n",
    "        '''\n",
    "        Taken from https://github.com/huggingface/transformers/blob/78b7debf56efb907c6af767882162050d4fbb294/src/transformers/modeling_utils.py#L1596\n",
    "        '''\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        elif isinstance(module, LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "            \n",
    "    def forward(self, inputs, conditions_labels=None, drugs_labels=None):\n",
    "        # inputs (B, 2, max_len)\n",
    "        # bert_pool (B, hidden)\n",
    "        _, conditions_bert_pool = self.bert(inputs[:, 0, :], torch.zeros(\n",
    "            (inputs.size(0), inputs.size(2))).long().to(inputs.device))\n",
    "        _, drugs_bert_pool = self.bert(inputs[:, 1, :], torch.zeros(\n",
    "            (inputs.size(0), inputs.size(2))).long().to(inputs.device))\n",
    "\n",
    "        conditions2condition = self.cls_conditions_1(conditions_bert_pool)\n",
    "        drug2condition = self.cls_drugs_1(drugs_bert_pool)\n",
    "        condition2drug = self.cls_conditions_2(conditions_bert_pool)\n",
    "        drug2drug = self.cls_drugs_2(drugs_bert_pool)\n",
    "        \n",
    "        # output logits\n",
    "        if drugs_labels is None or conditions_labels is None:\n",
    "            return torch.sigmoid(conditions2condition), torch.sigmoid(drug2condition), torch.sigmoid(condition2drug), torch.sigmoid(drug2drug)\n",
    "        else:\n",
    "            loss = F.binary_cross_entropy_with_logits(conditions2condition, conditions_labels) + \\\n",
    "                F.binary_cross_entropy_with_logits(drug2condition, conditions_labels) + \\\n",
    "                F.binary_cross_entropy_with_logits(condition2drug, drugs_labels) + \\\n",
    "                F.binary_cross_entropy_with_logits(drug2drug, drugs_labels)\n",
    "                \n",
    "            return loss, torch.sigmoid(conditions2condition), torch.sigmoid(drug2condition), torch.sigmoid(condition2drug), torch.sigmoid(drug2drug)\n",
    "        \n",
    "        \n",
    "    def from_pretrained(data, useGraph, outputFileName):\n",
    "        # Instantiate model.\n",
    "        model = BERT_Pretrain(data, 300, 0.4, useGraph)\n",
    "        \n",
    "        weights_path = os.path.join(\"\", outputFileName)\n",
    "        state_dict = torch.load(weights_path)\n",
    "\n",
    "        old_keys = []\n",
    "        new_keys = []\n",
    "        for key in state_dict.keys():\n",
    "            new_key = None\n",
    "            if 'gamma' in key:\n",
    "                new_key = key.replace('gamma', 'weight')\n",
    "            if 'beta' in key:\n",
    "                new_key = key.replace('beta', 'bias')\n",
    "            if new_key:\n",
    "                old_keys.append(key)\n",
    "                new_keys.append(new_key)\n",
    "        for old_key, new_key in zip(old_keys, new_keys):\n",
    "            state_dict[new_key] = state_dict.pop(old_key)\n",
    "\n",
    "        missing_keys = []\n",
    "        unexpected_keys = []\n",
    "        error_msgs = []\n",
    "        # copy state_dict so _load_from_state_dict can modify it\n",
    "        metadata = getattr(state_dict, '_metadata', None)\n",
    "        state_dict = state_dict.copy()\n",
    "        if metadata is not None:\n",
    "            state_dict._metadata = metadata\n",
    "\n",
    "        def load(module, prefix=''):\n",
    "            local_metadata = {} if metadata is None else metadata.get(\n",
    "                prefix[:-1], {})\n",
    "            module._load_from_state_dict(\n",
    "                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n",
    "            for name, child in module._modules.items():\n",
    "                if child is not None:\n",
    "                    load(child, prefix + name + '.')\n",
    "\n",
    "        load(model, prefix='' if hasattr(model, 'bert') else 'bert.')\n",
    "        return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b03bb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain(data, train_dataloader, eval_dataloader, outputFileName, usePretrainedModel, useGraph):\n",
    "    print(\"***** Running Pre-training *****\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available()\n",
    "                              else \"cpu\")\n",
    "\n",
    "    if usePretrainedModel:\n",
    "        model = BERT_Pretrain.from_pretrained(data, useGraph, outputFileName)\n",
    "    else:\n",
    "        model = BERT_Pretrain(data, 300, 0.4, useGraph)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    model_to_save = model.module if hasattr(\n",
    "        model, 'module') else model  # Only save the model it-self\n",
    "    dx_output_model_file = os.path.join(\n",
    "        '', outputFileName)\n",
    "    \n",
    "    optimizer = Adam(model.parameters(), lr=5e-4)\n",
    "    \n",
    "    dx_acc_best, rx_acc_best = 0, 0\n",
    "    acc_name = 'prauc'\n",
    "        \n",
    "    global_step = 0\n",
    "    for _ in range(5):\n",
    "        print(\"***** Running training *****\")\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, dx_labels, rx_labels = batch\n",
    "            loss, dx2dx, rx2dx, dx2rx, rx2rx = model(input_ids, dx_labels, rx_labels)\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += 1\n",
    "            nb_tr_steps += 1\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        global_step += 1\n",
    "        print('train/loss:', tr_loss / nb_tr_steps, \" epoch: \", global_step)\n",
    "        \n",
    "        print(\"***** Running eval *****\")\n",
    "        model.eval()\n",
    "        dx2dx_y_preds = []\n",
    "        rx2dx_y_preds = []\n",
    "        dx_y_trues = []\n",
    "\n",
    "        dx2rx_y_preds = []\n",
    "        rx2rx_y_preds = []\n",
    "        rx_y_trues = []\n",
    "        for batch in eval_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, dx_labels, rx_labels = batch\n",
    "            with torch.no_grad():\n",
    "                dx2dx, rx2dx, dx2rx, rx2rx = model(input_ids)\n",
    "                dx2dx_y_preds.append(t2n(dx2dx))\n",
    "                rx2dx_y_preds.append(t2n(rx2dx))\n",
    "                dx2rx_y_preds.append(t2n(dx2rx))\n",
    "                rx2rx_y_preds.append(t2n(rx2rx))\n",
    "\n",
    "                dx_y_trues.append(\n",
    "                    t2n(dx_labels))\n",
    "                rx_y_trues.append(\n",
    "                    t2n(rx_labels))\n",
    "\n",
    "        dx2dx_acc_container = metric_report(\n",
    "            np.concatenate(dx2dx_y_preds, axis=0), np.concatenate(dx_y_trues, axis=0))\n",
    "       \n",
    "        rx2dx_acc_container = metric_report(\n",
    "            np.concatenate(rx2dx_y_preds, axis=0), np.concatenate(dx_y_trues, axis=0))\n",
    "       \n",
    "        dx2rx_acc_container = metric_report(\n",
    "            np.concatenate(dx2rx_y_preds, axis=0), np.concatenate(rx_y_trues, axis=0))\n",
    "        \n",
    "        rx2rx_acc_container = metric_report(\n",
    "            np.concatenate(rx2rx_y_preds, axis=0), np.concatenate(rx_y_trues, axis=0))\n",
    "\n",
    "        for k, v in dx2dx_acc_container.items():\n",
    "            print('eval_dx2dx/', k, \": \", v, \" epoch: \", global_step)\n",
    "        for k, v in rx2dx_acc_container.items():\n",
    "            print('eval_rx2dx/', k, \": \", v, \" epoch: \", global_step)\n",
    "        for k, v in dx2rx_acc_container.items():\n",
    "            print('eval_dx2rx/', k, \": \", v, \" epoch: \", global_step)\n",
    "        for k, v in rx2rx_acc_container.items():\n",
    "            print('eval_rx2rx/', k, \": \", v, \" epoch: \", global_step)\n",
    "\n",
    "        if rx2rx_acc_container[acc_name] > dx_acc_best:\n",
    "            dx_acc_best = rx2rx_acc_container[acc_name]\n",
    "            # save model\n",
    "            torch.save(model_to_save.state_dict(),\n",
    "                       dx_output_model_file)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
