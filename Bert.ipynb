{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "12d70bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b68868",
   "metadata": {},
   "source": [
    "## The Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb366452",
   "metadata": {},
   "source": [
    "**Layer Normalization**: a technique commonly used in neural networks for normalizing the inputs to a layer.\n",
    "<br/>\n",
    "<br/>\n",
    "$$ Layer Normalization_i = weight * \\hat{x}_i + bias $$\n",
    "where\n",
    "$$ \\hat{x}_{i,k} = {{x_{i,k} - mean_i} \\over {\\sqrt{varience_i + \\epsilon}}} $$\n",
    "and\n",
    "$$ varience_i = {1 \\over K}{\\sum_{k=1}^{K} (x_{i,k}-\\mu_i)^2}$$\n",
    "\n",
    "<br/> \n",
    "refs: \n",
    "1. https://arxiv.org/pdf/1607.06450.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2aa888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-12):\n",
    "        \n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "        varience = torch.pow((x - mean), 2)\n",
    "        mean_varience = torch.mean(varience, dim=-1, keepdim=True)\n",
    "        x_hat = (x - mean) / torch.sqrt(mean_varience + self.variance_epsilon)\n",
    "        return self.weight * x_hat + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38db692e",
   "metadata": {},
   "source": [
    "**Sublayer Connection**: the function implemented by the sub-layer. The input x is normalized using LayerNorm and then passed into the sublayer. A dropout is applied to the output of the sublayer before it is added to the sub-layer input.\n",
    "<br/>\n",
    "<br/>\n",
    "Each layer has two sublayers: \n",
    "1. the Multi-Head Attention Layer \n",
    "2. the Position-wise Fully Connected Feed-Forward Network Layer\n",
    "\n",
    "<br/>\n",
    "refs: \n",
    "1. http://nlp.seas.harvard.edu/annotated-transformer/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03cd5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout_prob):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb46c766",
   "metadata": {},
   "source": [
    "**Scaled-Dot Product Attention**: an attention mechanism where the dot products are scaled down by $\\sqrt{d_k}$ where $d_k$ is the dimension of Q and K.\n",
    "\n",
    "$$ Attention(Q,K,V) = softmax({{QK^T} \\over {\\sqrt{d_k}}})V $$\n",
    "refs: \n",
    "1. https://arxiv.org/pdf/1706.03762.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "559c6c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        d_k = query.shape[0]\n",
    "        numerator = torch.matmul(query, torch.transpose(key, -2, -1))\n",
    "        scaled = numerator / (d_k**0.5)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scaled = scaled.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn = F.softmax(scaled, dim=-1)\n",
    "        \n",
    "        if dropout is not None:\n",
    "            attn = dropout(attn)\n",
    "        \n",
    "        output = torch.matmul(attn, value)\n",
    "        \n",
    "        return output, attn\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e28de5d",
   "metadata": {},
   "source": [
    "**Multi-Head Attention**: \n",
    "<br/> \n",
    "$$ MultiHead(Q,K,V) = Concat(head_1,..., head_h)W^O $$\n",
    "where\n",
    "$$ head_i = Attention(Q{W_i}^Q, K{W_i}^K, V{W_i}^V) $$\n",
    "\n",
    "refs: \n",
    "1. https://arxiv.org/pdf/1706.03762.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2853f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, dropout_prob):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0\n",
    "\n",
    "        self.d_k = hidden_size // num_heads\n",
    "        self.h = num_heads\n",
    "\n",
    "        linear_layers = []\n",
    "        for i in range(num_heads):\n",
    "            linear_layers.append(nn.Linear(hidden_size, hidden_size, bias=False))\n",
    "            \n",
    "        self.linear_layers = nn.ModuleList(linear_layers)\n",
    "        self.output_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        Q_new = torch.zeros_like(query)\n",
    "        K_new = torch.zeros_like(key)\n",
    "        V_new = torch.zeros_like(value)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            Q_new[i] = self.linear_layers[i](query[i])\n",
    "            K_new[i] = self.linear_layers[i](key[i])\n",
    "            V_new[i] = self.linear_layers[i](value[i])\n",
    "            \n",
    "        Q_new = Q_new.view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        K_new = K_new.view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        V_new = V_new.view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        x, attn = self.attention(Q_new, K_new, V_new, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "\n",
    "        return self.output_linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b3710e",
   "metadata": {},
   "source": [
    "**Position-wise Feed Foward**: consists of two linear transformations with an activation step inbetween\n",
    "<br/>\n",
    "<br/>\n",
    "In the paper Attention is All You Need, ReLU is used as the activation but in the G-BERT github page the guasssian error linear unit (GeLU) is used as the activation step. This is approximated as \n",
    "$$ GeLU(x) = 0.5x(1+tanh[\\sqrt{2/\\pi}(x+0.044715x^3)]) $$\n",
    "refs: \n",
    "1. https://arxiv.org/pdf/1606.08415.pdf  \n",
    "2. https://arxiv.org/pdf/1706.03762.pdf\n",
    "3. http://nlp.seas.harvard.edu/annotated-transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1013150c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, dropout_prob):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.w_2 = nn.Linear(intermediate_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gelu_result = 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "        return self.w_2(self.dropout(gelu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7f3f00",
   "metadata": {},
   "source": [
    "**Transformer Block**: this acts as the encoder and uses the two layers, multi-head attention and FNN, to encode the input. \n",
    "\n",
    "refs:\n",
    "1. http://nlp.seas.harvard.edu/annotated-transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b2036524",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, num_heads, intermediate_size, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttention(hidden_size, num_heads, dropout_prob)\n",
    "        self.feed_forward = PositionwiseFeedForward(hidden_size, intermediate_size, dropout_prob)\n",
    "        self.input_sublayer = SublayerConnection(hidden_size, dropout_prob)\n",
    "        self.output_sublayer = SublayerConnection(hidden_size, dropout_prob)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attention_layer = lambda y: self.attention.forward(y, y, y, mask=mask)\n",
    "        fnn_layer = self.feed_forward\n",
    "        x = self.input_sublayer(x, attention_layer)\n",
    "        x = self.output_sublayer(x, fnn_layer)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60505d52",
   "metadata": {},
   "source": [
    "**Bert Embeddings**: BERT embeddings usually include word embeddings, token embeddings, and position emmbeddings, G-BERT does not include position embeddings because we are using medical codes as our input which do not have a specific order within a given visit\n",
    "<br/>\n",
    "<br/>\n",
    "refs:\n",
    "1. https://www.ijcai.org/proceedings/2019/0825.pdf\n",
    "2. https://arxiv.org/pdf/1810.04805.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd038e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, hidden_size, dropout_prob):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        self.segment_embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.token_embeddings = nn.Embedding(2, hidden_size)\n",
    "\n",
    "        self.LayerNorm = LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, token_ids=None):\n",
    "        if token_ids is None:\n",
    "            token_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        segment_embeddings = self.segment_embeddings(input_ids)\n",
    "\n",
    "        embeddings = segment_embeddings + self.token_embeddings(token_ids)\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
